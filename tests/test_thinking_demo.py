"""
æ€è€ƒè¿‡ç¨‹æ¼”ç¤ºæµ‹è¯• - æµ‹è¯•æµå¼è·å–å’ŒåŒºåˆ†OpenAIæ¨¡å‹çš„æ€è€ƒè¿‡ç¨‹

è¯¥æµ‹è¯•æ–‡ä»¶ï¼š
1. å¯ä»¥ç›´æ¥è¿è¡Œï¼Œæ— éœ€ç›¸å¯¹å¯¼å…¥
2. æä¾›å®Œæ•´çš„é”™è¯¯å¤„ç†
3. åŒ…å«è¯¦ç»†çš„æµ‹è¯•ç”¨ä¾‹
"""

import asyncio
import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

try:
    from agentlz.config.settings import get_settings
    from agentlz.core.model_factory import get_model
    from agentlz.tools.streaming_processor import StreamingProcessor, create_thinking_prompt
    from agentlz.core.logger import setup_logging
    from langchain_core.messages import HumanMessage
except ImportError as e:
    print(f"å¯¼å…¥é”™è¯¯: {e}")
    print("è¯·ç¡®ä¿å·²å®‰è£…æ‰€æœ‰ä¾èµ–: pip install -r requirements.txt")
    sys.exit(1)


class ThinkingDemoTest:
    """
    æ€è€ƒè¿‡ç¨‹æ¼”ç¤ºæµ‹è¯•ç±»
    
    å‚æ•°:
        None
        
    å±æ€§:
        settings: åº”ç”¨é…ç½®
        logger: æ—¥å¿—è®°å½•å™¨
        processor: æµå¼å¤„ç†å™¨
    """
    
    def __init__(self):
        """åˆå§‹åŒ–æµ‹è¯•ç±»"""
        try:
            self.settings = get_settings()
            self.logger = setup_logging(self.settings.log_level)
            self.processor = StreamingProcessor()
            self.logger.info("æ€è€ƒè¿‡ç¨‹æ¼”ç¤ºæµ‹è¯•åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    async def test_stream_thinking_response(self, query: str) -> dict:
        """
        æµ‹è¯•æµå¼è·å–æ€è€ƒè¿‡ç¨‹å“åº”
        
        å‚æ•°:
            query: ç”¨æˆ·æŸ¥è¯¢å­—ç¬¦ä¸²
            
        è¿”å›å€¼:
            dict: åŒ…å«æµ‹è¯•ç»“æœçš„å­—å…¸
            
        å¼‚å¸¸:
            å¯èƒ½æŠ›å‡ºæ¨¡å‹è°ƒç”¨æˆ–æµå¼å¤„ç†ç›¸å…³çš„å¼‚å¸¸
        """
        try:
            # è·å–æµå¼æ¨¡å‹
            model = get_model(self.settings, streaming=True)
            if not model:
                return {
                    "success": False,
                    "error": "æ¨¡å‹é…ç½®æ— æ•ˆï¼Œæ— æ³•åˆ›å»ºæ¨¡å‹å®ä¾‹",
                    "query": query
                }
            
            # åˆ›å»ºåŒ…å«æ€è€ƒè¿‡ç¨‹è¦æ±‚çš„æç¤ºè¯
            prompt = create_thinking_prompt(query)
            
            # å‡†å¤‡æ¶ˆæ¯
            messages = [HumanMessage(content=prompt)]
            
            # æ”¶é›†æµå¼å“åº”çš„å˜é‡
            thinking_chunks = []
            answer_chunks = []
            all_chunks = []
            
            # å®šä¹‰chunkå¤„ç†å›è°ƒ
            async def process_chunk(chunk: str):
                all_chunks.append(chunk)
                
                # å®æ—¶åˆ†ç±»å†…å®¹ç±»å‹
                content_type = self.processor.classify_content_type(chunk)
                
                if content_type == self.processor.ContentType.THINKING:
                    thinking_chunks.append(chunk)
                elif content_type == self.processor.ContentType.FINAL_ANSWER:
                    answer_chunks.append(chunk)
            
            # è°ƒç”¨æµå¼æ¨¡å‹
            response_stream = model.astream(messages)
            
            # å¤„ç†æµå¼å“åº”
            thinking_process, final_answer = await self.processor.process_streaming_response(
                response_stream, process_chunk
            )
            
            # å¦‚æœæ²¡æœ‰ä»æµå¼å¤„ç†ä¸­æå–åˆ°å†…å®¹ï¼Œå°è¯•ä»å®Œæ•´å“åº”ä¸­æå–
            full_response = ''.join(all_chunks)
            if not thinking_process or not final_answer:
                structured = self.processor.extract_structured_response(full_response)
                thinking_process = structured['thinking_process'] or thinking_process
                final_answer = structured['final_answer'] or final_answer
            
            return {
                "success": True,
                "query": query,
                "thinking_process": thinking_process,
                "final_answer": final_answer,
                "thinking_chunks_count": len(thinking_chunks),
                "answer_chunks_count": len(answer_chunks),
                "total_chunks": len(all_chunks),
                "has_thinking": bool(thinking_process),
                "has_answer": bool(final_answer)
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "query": query
            }
    
    async def test_basic_response(self, query: str) -> dict:
        """
        æµ‹è¯•åŸºæœ¬å“åº”ï¼ˆéæµå¼ï¼‰
        
        å‚æ•°:
            query: ç”¨æˆ·æŸ¥è¯¢å­—ç¬¦ä¸²
            
        è¿”å›å€¼:
            dict: åŒ…å«æµ‹è¯•ç»“æœçš„å­—å…¸
        """
        try:
            # è·å–æ™®é€šæ¨¡å‹
            model = get_model(self.settings, streaming=False)
            if not model:
                return {
                    "success": False,
                    "error": "æ¨¡å‹é…ç½®æ— æ•ˆ",
                    "query": query
                }
            
            # åˆ›å»ºæç¤ºè¯
            prompt = create_thinking_prompt(query)
            messages = [HumanMessage(content=prompt)]
            
            # è°ƒç”¨æ¨¡å‹
            response = model.invoke(messages)
            
            # æå–å†…å®¹
            if hasattr(response, 'content'):
                content = response.content
            else:
                content = str(response)
            
            # æå–ç»“æ„åŒ–å“åº”
            structured = self.processor.extract_structured_response(content)
            
            return {
                "success": True,
                "query": query,
                "thinking_process": structured['thinking_process'],
                "final_answer": structured['final_answer'],
                "response_length": len(content),
                "has_thinking": bool(structured['thinking_process']),
                "has_answer": bool(structured['final_answer'])
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "query": query
            }


async def run_demo_tests():
    """è¿è¡Œæ¼”ç¤ºæµ‹è¯•"""
    print("=== æ€è€ƒè¿‡ç¨‹æ¼”ç¤ºæµ‹è¯• ===\n")
    
    try:
        test = ThinkingDemoTest()
        print("âœ… æµ‹è¯•åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æµ‹è¯•åˆå§‹åŒ–å¤±è´¥: {e}")
        return
    
    # æµ‹è¯•æŸ¥è¯¢åˆ—è¡¨
    test_queries = [
        "å¦‚ä½•è®¡ç®—åœ†çš„é¢ç§¯ï¼Ÿ",
        "è¯·è§£é‡Šäººå·¥æ™ºèƒ½çš„åŸºæœ¬æ¦‚å¿µ",
        "å¸®æˆ‘åˆ¶å®šä¸€ä¸ªå­¦ä¹ è®¡åˆ’",
        "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"
    ]
    
    for i, query in enumerate(test_queries, 1):
        print(f"\n--- æµ‹è¯• {i}: {query} ---")
        
        # æµ‹è¯•æµå¼å“åº”
        print("ğŸ”„ æµ‹è¯•æµå¼å“åº”...")
        stream_result = await test.test_stream_thinking_response(query)
        
        if stream_result["success"]:
            print("âœ… æµå¼æµ‹è¯•æˆåŠŸ")
            print(f"   æ€è€ƒè¿‡ç¨‹: {'æœ‰' if stream_result['has_thinking'] else 'æ— '}")
            print(f"   æœ€ç»ˆç­”æ¡ˆ: {'æœ‰' if stream_result['has_answer'] else 'æ— '}")
            print(f"   åŒºå—ç»Ÿè®¡: æ€è€ƒ{stream_result['thinking_chunks_count']}, ç­”æ¡ˆ{stream_result['answer_chunks_count']}")
            
            if stream_result['thinking_process']:
                print(f"   ğŸ“ æ€è€ƒå†…å®¹: {stream_result['thinking_process'][:100]}...")
            if stream_result['final_answer']:
                print(f"   ğŸ’¡ æœ€ç»ˆç­”æ¡ˆ: {stream_result['final_answer'][:100]}...")
        else:
            print(f"âŒ æµå¼æµ‹è¯•å¤±è´¥: {stream_result['error']}")
        
        # æµ‹è¯•åŸºæœ¬å“åº”
        print("ğŸ“‹ æµ‹è¯•åŸºæœ¬å“åº”...")
        basic_result = await test.test_basic_response(query)
        
        if basic_result["success"]:
            print("âœ… åŸºæœ¬æµ‹è¯•æˆåŠŸ")
            print(f"   å“åº”é•¿åº¦: {basic_result['response_length']} å­—ç¬¦")
            print(f"   æ€è€ƒè¿‡ç¨‹: {'æœ‰' if basic_result['has_thinking'] else 'æ— '}")
            print(f"   æœ€ç»ˆç­”æ¡ˆ: {'æœ‰' if basic_result['has_answer'] else 'æ— '}")
        else:
            print(f"âŒ åŸºæœ¬æµ‹è¯•å¤±è´¥: {basic_result['error']}")
        
        print("-" * 50)
    
    print("\n=== æµ‹è¯•å®Œæˆ ===")


def check_environment():
    """æ£€æŸ¥ç¯å¢ƒé…ç½®"""
    print("ğŸ” æ£€æŸ¥ç¯å¢ƒé…ç½®...")
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡æ–‡ä»¶
    env_file = project_root / ".env"
    if env_file.exists():
        print("âœ… .env æ–‡ä»¶å­˜åœ¨")
    else:
        print("âš ï¸  .env æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å¤åˆ¶ .env.example å¹¶é…ç½®APIå¯†é’¥")
    
    # æ£€æŸ¥è®¾ç½®
    try:
        settings = get_settings()
        has_openai_key = bool(settings.openai_api_key)
        has_custom_key = bool(settings.chatopenai_api_key)
        
        print(f"ğŸ”‘ OpenAI APIå¯†é’¥: {'å·²é…ç½®' if has_openai_key else 'æœªé…ç½®'}")
        print(f"ğŸ”‘ è‡ªå®šä¹‰APIå¯†é’¥: {'å·²é…ç½®' if has_custom_key else 'æœªé…ç½®'}")
        print(f"ğŸ¤– æ¨¡å‹åç§°: {settings.model_name}")
        
        if not has_openai_key and not has_custom_key:
            print("âŒ é”™è¯¯: æ²¡æœ‰é…ç½®ä»»ä½•APIå¯†é’¥")
            return False
            
    except Exception as e:
        print(f"âŒ é…ç½®æ£€æŸ¥å¤±è´¥: {e}")
        return False
    
    return True


if __name__ == "__main__":
    # æ£€æŸ¥ç¯å¢ƒ
    if not check_environment():
        print("\nè¯·å…ˆé…ç½®ç¯å¢ƒ:")
        print("1. å¤åˆ¶ .env.example ä¸º .env")
        print("2. åœ¨ .env ä¸­é…ç½® OPENAI_API_KEY æˆ– CHATOPENAI_API_KEY")
        print("3. é‡æ–°è¿è¡Œæµ‹è¯•")
        sys.exit(1)
    
    # è¿è¡Œæµ‹è¯•
    try:
        asyncio.run(run_demo_tests())
    except KeyboardInterrupt:
        print("\næµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\næµ‹è¯•è¿è¡Œé”™è¯¯: {e}")
        print("\nè°ƒè¯•å»ºè®®:")
        print("1. æ£€æŸ¥APIå¯†é’¥æ˜¯å¦æ­£ç¡®")
        print("2. æ£€æŸ¥ç½‘ç»œè¿æ¥")
        print("3. æŸ¥çœ‹è¯¦ç»†é”™è¯¯ä¿¡æ¯")
        import traceback
        traceback.print_exc()